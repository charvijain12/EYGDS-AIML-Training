 1. What is Prompting?

Prompting means giving instructions or examples to an LLM in natural language (or structured text) so that it produces the desired output.
Itâ€™s essentially the art of communicating effectively with an LLM.

A prompt can include:
	â€¢	A question or instruction (â€œWrite a poem about the oceanâ€)
	â€¢	Context or examples (â€œTranslate this text to Frenchâ€)
	â€¢	Constraints (â€œUse fewer than 100 wordsâ€)

Prompting directly affects model behavior â€” the better the prompt, the more relevant and accurate the response.

---------------------------------------------------------------------------------------------------------------------------------------------------------------

2. Zero-shot Prompting

Definition:
Giving the model a task without any examples â€” you rely entirely on the modelâ€™s pretrained knowledge.

Example:

â€œClassify the sentiment of this review: The movie was amazing!â€

The model must infer that this is a positive sentiment just from its general knowledge.

Use case:
When the model already understands the task type (like summarization, translation, classification, etc.).

--------------------------------------------------------------------------------------------------------------------------------------------------------------

3. Few-shot Prompting

Definition:
You show the model a few examples of the task before asking it to perform it.
This helps the model â€œinfer the patternâ€ or â€œtask formatâ€.

Use case:
Useful when the model isnâ€™t fine-tuned for a task, or when you want to establish a specific tone, style, or format.

â¸»

ğŸ”— 4. Chain-of-Thought (CoT) Prompting

Definition:
Encourages the model to reason step-by-step before giving the final answer.
This is especially useful for math, logic, or reasoning tasks.

Example:

â€œLetâ€™s think step by step:
A train travels 60 km in 1 hour. How long will it take to travel 180 km?â€

The model might respond:

â€œ60 km in 1 hour means 60 km/hour.
180 Ã· 60 = 3.
So it will take 3 hours.â€

Use case:
When reasoning or multi-step logic is required.

â¸»

ğŸ›ï¸ 5. Prompt Tuning

Definition:
Instead of manually writing prompts, we train small, learnable prompt vectors (soft prompts) that guide the modelâ€™s behavior for a specific task.

Think of it as fine-tuning the prompt, not the model.

Two types:
	1.	Soft Prompt Tuning:
	â€¢	The â€œpromptâ€ is a small set of continuous embeddings (not human-readable text) learned through gradient descent.
	â€¢	Only the prompt is trained; the main model weights are frozen.
	2.	Prefix Tuning / P-tuning:
	â€¢	Similar idea â€” a learned prefix is prepended to the modelâ€™s inputs to steer behavior.

Example use case:
Instead of retraining a 175B-parameter model, you can train a 10K-parameter soft prompt for each downstream task â€” efficient and lightweight.
